{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGxPUOWqQgaw"
   },
   "source": [
    "This note book contains the scripts for preprocessing financil data from a video, converting them from video to frames and then sending them to VERTEX GEMINI API for extracting data. using offcial Vertex SDK for video uploading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PR8jul5bQga0",
    "outputId": "2b87166f-9f2a-4a05-bc30-9f6d4527662f"
   },
   "outputs": [],
   "source": [
    "# # only run if you want to To download video from s3 or any downloadable link convert video into frames and save it into a folder. read the last 2 instruction paras\n",
    "# import cv2\n",
    "# import os\n",
    "# import urllib.request\n",
    "\n",
    "# def download_video(url, local_path):\n",
    "#     try:\n",
    "#         urllib.request.urlretrieve(url, local_path)\n",
    "#         print(f\"Video downloaded from {url} to: {local_path}\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error downloading video: {e}\")\n",
    "#         return False\n",
    "\n",
    "# def save_frames_from_video(video_path, output_folder, frame_interval=1):\n",
    "#     if not os.path.exists(output_folder):\n",
    "#         os.makedirs(output_folder)\n",
    "\n",
    "#     video = cv2.VideoCapture(video_path)\n",
    "#     if not video.isOpened():\n",
    "#         print(\"Error: Could not open video.\")\n",
    "#         return\n",
    "\n",
    "#     frame_count = 0\n",
    "#     while True:\n",
    "#         success, frame = video.read()\n",
    "#         if not success:\n",
    "#             break\n",
    "\n",
    "#         if frame_count % frame_interval == 0:\n",
    "#             frame_file = os.path.join(output_folder, f\"frame_{frame_count}.jpg\")\n",
    "#             cv2.imwrite(frame_file, frame)\n",
    "\n",
    "#         frame_count += 1\n",
    "\n",
    "#     video.release()\n",
    "#     print(f\"Total frames saved: {frame_count}\")\n",
    "\n",
    "# # System prompt for video URL\n",
    "# video_url = input(\"Enter the URL of the video to download: \")\n",
    "# local_video_path = \"downloaded_video.mp4\"  # Local path to save the downloaded video\n",
    "\n",
    "# # User prompt for frame extraction interval\n",
    "# frame_interval = int(input(\"Enter the frame extraction interval (e.g., 1 for every frame, 10 for every 10th frame, etc.): \"))\n",
    "\n",
    "# # Download video from the provided URL\n",
    "# if download_video(video_url, local_video_path):\n",
    "#     # Process and save frames from the downloaded video\n",
    "#     output_directory = \"./2Day frames\"  # Replace with your desired output folder\n",
    "#     save_frames_from_video(local_video_path, output_directory)\n",
    "    \n",
    "# # # Video URL\n",
    "# # video_url = \"https://0-source-principal-administrative.s3.amazonaws.com/0-public-data/SPX_2D.mp4\"\n",
    "# # local_video_path = \"2day Data.mp4\"  # Local path to save the downloaded video\n",
    "\n",
    "# # Download video from the provided URL\n",
    "# if download_video(video_url, local_video_path):\n",
    "#     # Process and save frames from the downloaded video\n",
    "#     output_directory = \"./2Day frames\"  # Replace with your desired output folder\n",
    "#     save_frames_from_video(local_video_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def download_video(url, local_path):\n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"Video already exists at: {local_path}\")\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, local_path)\n",
    "        print(f\"Video downloaded from {url} to: {local_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading video: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_frames_from_video(video_path, output_folder, frame_interval=1):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    if not video.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_file = os.path.join(output_folder, f\"frame_{frame_count}.jpg\")\n",
    "            cv2.imwrite(frame_file, frame)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    video.release()\n",
    "    print(f\"Total frames saved: {frame_count}\")\n",
    "\n",
    "# System prompt for video URL\n",
    "video_url = input(\"Enter the URL of the video to download: \")\n",
    "local_video_path = \"downloaded_video.mp4\"  # Local path to save the downloaded video\n",
    "\n",
    "# User prompt for frame extraction interval\n",
    "frame_interval = int(input(\"Enter the frame extraction interval (e.g., 1 for every frame, 10 for every 10th frame, etc.): \"))\n",
    "\n",
    "# Download video from the provided URL if not already downloaded\n",
    "if download_video(video_url, local_video_path):\n",
    "    # Process and save frames from the downloaded video\n",
    "    output_directory = \"./2Day frames\"  # Replace with your desired output folder\n",
    "    save_frames_from_video(local_video_path, output_directory, frame_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytesseract\n",
    "# pytesseract.pytesseract.tesseract_cmd = (\n",
    "#     r'/usr/bin/tesseract'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "mSGrIjCRQga4",
    "outputId": "993e15d7-3d65-40fe-9973-0cc8e3ee38a1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "# Path to the folder containing images\n",
    "folder_path = '2Day frames'\n",
    "\n",
    "# This function uses Tesseract to extract text from a specified region in an image\n",
    "def extract_date_from_image(image_path, region):\n",
    "    with Image.open(image_path) as img:\n",
    "        cropped_img = img.crop(region)\n",
    "        text = pytesseract.image_to_string(cropped_img, lang='eng', config='--psm 6')\n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "# Define the region where the date is located\n",
    "date_region = (142, 50, 267, 75)\n",
    "\n",
    "# Get all image file paths\n",
    "image_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith(('.jpg', '.png'))]\n",
    "\n",
    "# Create a dictionary to group images by date\n",
    "date_groups = {}\n",
    "\n",
    "# Process each image\n",
    "for image_path in image_paths:\n",
    "    date_text = extract_date_from_image(image_path, date_region)\n",
    "    if date_text not in date_groups:\n",
    "        date_groups[date_text] = []\n",
    "    date_groups[date_text].append(image_path)\n",
    "\n",
    "# Now you have a dictionary where the key is the date and the value is a list of image paths with that date\n",
    "for date, images in date_groups.items():\n",
    "    print(f\"Date: {date}, Images: {images}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABOVE CODE WITH 10 MIN TIMEOUT, USE THIS BLOCK IF PROCESSING IS SLOW ABOVE\n",
    "import os\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import time\n",
    "\n",
    "# Path to the folder containing images\n",
    "folder_path = '2Day frames'\n",
    "\n",
    "# This function uses Tesseract to extract text from a specified region in an image\n",
    "def extract_date_from_image(image_path, region):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            cropped_img = img.crop(region)\n",
    "            text = pytesseract.image_to_string(cropped_img, lang='eng', config='--psm 6')\n",
    "            return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the region where the date is located\n",
    "date_region = (142, 50, 267, 75)\n",
    "\n",
    "# Get all image file paths\n",
    "image_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith(('.jpg', '.png'))]\n",
    "\n",
    "# Create a dictionary to group images by date\n",
    "date_groups = {}\n",
    "\n",
    "# Set a timeout (in seconds)\n",
    "timeout = 600  # 10 minutes\n",
    "\n",
    "# Process each image\n",
    "start_time = time.time()\n",
    "for image_path in image_paths:\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > timeout:\n",
    "        print(f\"Timeout reached. Exiting.\")\n",
    "        break\n",
    "\n",
    "    date_text = extract_date_from_image(image_path, date_region)\n",
    "    if date_text is not None:\n",
    "        if date_text not in date_groups:\n",
    "            date_groups[date_text] = []\n",
    "        date_groups[date_text].append(image_path)\n",
    "\n",
    "# Now you have a dictionary where the key is the date and the value is a list of image paths with that date\n",
    "for date, images in date_groups.items():\n",
    "    print(f\"Date: {date}, Images: {images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAfEFMmnQga5",
    "outputId": "d58be385-81dd-4a65-d6f9-1088b5a7dd66"
   },
   "outputs": [],
   "source": [
    "new = {}\n",
    "for date, images in date_groups.items():\n",
    "    if images:  # Check if there are any images for the date\n",
    "        new[date] = images[0]  # Add only the first image of the current date\n",
    "\n",
    "for date, image in new.items():\n",
    "    print( {date}, {image})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWb53_05Qga5",
    "outputId": "bc598ecd-539d-4fc7-f0b0-ff58d9ed5841"
   },
   "outputs": [],
   "source": [
    "#only run to make a copy of extracted unique images to new folder\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Base directory where new folders will be created\n",
    "base_directory = '2D extracted frames'  # Replace with your desired path\n",
    "\n",
    "# Iterate over the dictionary\n",
    "for date, image_path in new.items():\n",
    "    # Create a valid directory name for the date\n",
    "    # Replace invalid characters for folder names (like '/') with an underscore or another valid character\n",
    "    folder_name = date.replace(':', '_').replace('/', '_')\n",
    "    directory_path = os.path.join(base_directory)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    # Define the destination path for the image\n",
    "    destination_path = os.path.join(directory_path, os.path.basename(image_path))\n",
    "\n",
    "    # Copy the image to the new location\n",
    "    shutil.copy(image_path, destination_path)\n",
    "\n",
    "    print(f\"Copied {image_path} to {destination_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=\"crafty-ring-411412\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dmnsAwVoQga6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")\n",
    "\n",
    "import vertexai\n",
    "vertexai.init(project=\"crafty-ring-411412\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yPHgjqn5Qga6"
   },
   "outputs": [],
   "source": [
    "multimodal_model = GenerativeModel(\"gemini-pro-vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sNxkgOAeQga7",
    "outputId": "bcb35ed3-066d-42d5-b349-dc5ccfa75b13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frame_0.jpg', 'frame_133.jpg', 'frame_166.jpg', 'frame_205.jpg', 'frame_229.jpg', 'frame_254.jpg', 'frame_292.jpg', 'frame_321.jpg', 'frame_345.jpg', 'frame_367.jpg', 'frame_389.jpg', 'frame_413.jpg', 'frame_435.jpg', 'frame_464.jpg', 'frame_487.jpg', 'frame_494.jpg', 'frame_504.jpg', 'frame_515.jpg', 'frame_537.jpg', 'frame_543.jpg', 'frame_564.jpg', 'frame_586.jpg', 'frame_605.jpg', 'frame_630.jpg', 'frame_653.jpg', 'frame_678.jpg', 'frame_704.jpg', 'frame_729.jpg', 'frame_751.jpg', 'frame_770.jpg', 'frame_795.jpg', 'frame_821.jpg', 'frame_844.jpg', 'frame_875.jpg', 'frame_899.jpg', 'frame_916.jpg', 'frame_936.jpg', 'frame_955.jpg', 'frame_973.jpg', 'frame_1000.jpg', 'frame_1015.jpg', 'frame_1038.jpg', 'frame_1067.jpg', 'frame_1109.jpg', 'frame_1144.jpg', 'frame_1164.jpg', 'frame_1191.jpg', 'frame_1221.jpg', 'frame_1246.jpg', 'frame_1248.jpg', 'frame_1266.jpg', 'frame_1298.jpg', 'frame_1318.jpg', 'frame_1338.jpg', 'frame_1355.jpg', 'frame_1375.jpg', 'frame_1395.jpg', 'frame_1415.jpg', 'frame_1440.jpg', 'frame_1464.jpg', 'frame_1483.jpg', 'frame_1505.jpg', 'frame_1538.jpg', 'frame_1571.jpg', 'frame_1594.jpg', 'frame_1619.jpg', 'frame_1646.jpg', 'frame_1664.jpg', 'frame_1685.jpg', 'frame_1709.jpg', 'frame_1736.jpg', 'frame_1755.jpg', 'frame_1774.jpg', 'frame_1795.jpg', 'frame_1822.jpg', 'frame_1845.jpg', 'frame_1884.jpg', 'frame_1906.jpg', 'frame_1927.jpg', 'frame_1947.jpg', 'frame_1967.jpg', 'frame_1995.jpg', 'frame_2018.jpg', 'frame_2052.jpg', 'frame_2081.jpg', 'frame_2116.jpg', 'frame_2142.jpg', 'frame_2161.jpg', 'frame_2185.jpg', 'frame_2211.jpg', 'frame_2243.jpg', 'frame_2266.jpg', 'frame_2307.jpg', 'frame_2324.jpg', 'frame_2351.jpg', 'frame_2374.jpg', 'frame_2395.jpg', 'frame_2429.jpg', 'frame_2456.jpg', 'frame_2488.jpg', 'frame_2515.jpg', 'frame_2575.jpg', 'frame_2604.jpg', 'frame_2632.jpg', 'frame_2651.jpg', 'frame_2678.jpg', 'frame_2696.jpg', 'frame_2724.jpg', 'frame_2748.jpg', 'frame_2772.jpg', 'frame_2789.jpg', 'frame_2809.jpg', 'frame_2828.jpg', 'frame_2853.jpg', 'frame_2873.jpg', 'frame_2909.jpg', 'frame_2929.jpg', 'frame_2973.jpg', 'frame_2996.jpg', 'frame_3016.jpg', 'frame_3038.jpg', 'frame_3060.jpg', 'frame_3090.jpg', 'frame_3125.jpg', 'frame_3155.jpg', 'frame_3174.jpg', 'frame_3193.jpg', 'frame_3212.jpg', 'frame_3241.jpg', 'frame_3265.jpg', 'frame_3282.jpg', 'frame_3308.jpg', 'frame_3335.jpg', 'frame_3364.jpg', 'frame_3408.jpg', 'frame_3409.jpg', 'frame_3410.jpg', 'frame_3411.jpg', 'frame_3412.jpg', 'frame_3413.jpg']\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "#Sorting the frame names and Extracting frames from the folder.\n",
    "import os\n",
    "import re\n",
    "from vertexai.preview.generative_models import Image\n",
    "\n",
    "def extract_number(file_name):\n",
    "    # Extract the numeric part from the filename\n",
    "    match = re.search(r'\\d+', file_name)\n",
    "    return int(match.group()) if match else 0\n",
    "\n",
    "# Directory containing your images\n",
    "directory = \"./2D extracted frames\"\n",
    "\n",
    "# List all image filenames in the directory\n",
    "image_filenames = [filename for filename in os.listdir(directory) if filename.endswith(\".jpg\")]\n",
    "\n",
    "# Sort the image filenames by their numeric part\n",
    "sorted_filenames = sorted(image_filenames, key=lambda x: extract_number(x))\n",
    "print(sorted_filenames)\n",
    "# Load images in sorted order\n",
    "images = [Image.load_from_file(os.path.join(directory, filename)) for filename in sorted_filenames]\n",
    "\n",
    "print(len(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GpVDiF2gQga8",
    "outputId": "efd3c1ae-8979-488b-c6dd-8bfe00083fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Date         | Open  | High | Low  | Close |\n",
      "|-------------|-------|-------|-------|-------|\n",
      "| 12/22/2022   | 3834.36 | 3846.65 | 3812.22 | 3829.25 |\n",
      "| 12/23/2022   | 3829.56 | 3858.19 | 3780.78 | 3848.28 |\n",
      "| 12/27/2022   | 3855.29 | 3893.46 | 3794.33 | 3852.97 |\n",
      "| 12/28/2022   | 3836.08 | 3874.16 | 3802.42 | 3835.08 |\n",
      "| 12/29/2022   | 3839.74 | 3905.93 | 3824.12 | 3890.09 |\n",
      "| 12/30/2022   | 3910.82 | 3950.57 | 3897.29 | 3915.25 |\n",
      "| 1/3/2023     | 3906.6  | 4013.23 | 3896.54 | 3999.09 |\n",
      "| 1/4/2023     | 3928.28 | 4015.48 | 3926.59 | 3982.86 |\n",
      "| 1/5/2023     | 3911.84 | 3972.96 | 3885.54 | 3927.61 |\n",
      "| 1/6/2023     | 3978.14 | 4039.23 | 3957.64 | 4016.95 |\n",
      "| 1/9/2023     | 4032.71 | 4061.57 | 3949.06 | 4060.43 |\n",
      "| 1/10/2023    | 4055.72 | 4092.41 | 4015.55 | 4017.77 |\n",
      "| 1/11/2023    | 4020.85 | 4148.95 | 4020.44 | 4119.21 |\n",
      "| 1/12/2023    | 4158.68 | 4195.44 | 4123.36 | 4136.48 |\n"
     ]
    }
   ],
   "source": [
    "#multiple images per request\n",
    "New = images[:15]\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.4,\n",
    "    top_k=10,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=2048,\n",
    ")\n",
    "generative_multimodal_model = GenerativeModel(\"gemini-pro-vision\")\n",
    "response = generative_multimodal_model.generate_content([ *New, \"Parse values of Date, open, high, low, close from EVERY ONE these images of financial data. provide data in a table.\"], generation_config=generation_config)\n",
    "print(response.candidates[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Q74v0lWPQga8",
    "outputId": "789ce6b3-6f5c-484a-c992-9e966df61772"
   },
   "outputs": [],
   "source": [
    "# #one image per repuest\n",
    "# import time\n",
    "# for image in New:\n",
    "#     response = generative_multimodal_model.generate_content([*New,\"parse date, open, high, low, close from this image and make a table\", image])\n",
    "#     print(response.candidates[0].text)\n",
    "#     time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GTI0i9TZQga9"
   },
   "outputs": [],
   "source": [
    "#video preprocessing\n",
    "import base64\n",
    "\n",
    "def convert_video_to_base64(video_file_path):\n",
    "    # Read the video file\n",
    "    with open(video_file_path, 'rb') as video_file:\n",
    "        video_data = video_file.read()\n",
    "\n",
    "    # Encode the video data in base64\n",
    "    base64_encoded_data = base64.b64encode(video_data)\n",
    "\n",
    "    # Convert to a string for easier handling\n",
    "    base64_encoded_string = base64_encoded_data.decode('utf-8')\n",
    "\n",
    "    return base64_encoded_string\n",
    "\n",
    "# Example usage\n",
    "video_base64 = convert_video_to_base64('C:/Users/SUYASH/Downloads/downloaded_video.mp4') #replace video location here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cK7pHBNaQga9",
    "outputId": "751d3b97-1ded-4be4-e530-ce732a54cddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------Response--------\n",
      " The video shows the following readings:\n",
      "\n",
      "Date\tOpen\tHigh\tLow\tClose\n",
      "2023-03-08\t3846.65\t4126.43\t4039.56\t4377.26\n",
      "2023-03-09\t4126.43\t4439.56\t4312.38\t4772.94\n",
      "2023-03-10\t4439.56\t4567.89\t4439.56\t4512.36\n",
      "2023-03-13\t4567.89\t4634.50\t4512.36\t4600.12\n",
      "2023-03-14\t4634.50\t4754.23\t4600.12\t4723.45"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "parse all different reading of date, open, high, low, close, from this video.\n",
    "\"\"\"\n",
    "generation_config={\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"temperature\": 0.4,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 32\n",
    "    }\n",
    "video = Part.from_data(\n",
    "    data=video_base64,\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "contents = [prompt, video]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
